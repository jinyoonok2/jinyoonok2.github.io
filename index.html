<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jinyoon Kim's Webpage</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom styles can be added here if needed */
        body {
            font-family: 'Inter', sans-serif; /* Using Inter font */
        }
        .link-button {
            text-decoration: underline;
            color: #3b82f6; /* Tailwind blue-500 */
        }
        .link-button:hover {
            color: #1d4ed8; /* Tailwind blue-700 */
        }
        .navbar {
             background-color: #1f2937; /* Tailwind gray-800 */
        }
        .section-heading {
             border-bottom: 2px solid #e5e7eb; /* Tailwind gray-200 */
             padding-bottom: 0.5rem; /* Equivalent to pb-2 */
             margin-bottom: 1rem; /* Equivalent to mb-4 */
        }
        /* Style for project images to maintain aspect ratio */
        .project-img {
            aspect-ratio: 16 / 9; /* Common aspect ratio, adjust as needed */
            object-fit: cover; /* Cover the container */
            width: 100%;
            height: auto;
            border-radius: 0.375rem; /* Tailwind rounded-md */
        }
        /* Custom class for profile image container */
        .profile-img-container {
             width: 50%; /* Half width on small screens */
        }
        @media (min-width: 768px) { /* Corresponds to Tailwind's 'md' breakpoint */
            .profile-img-container {
                 width: 12.5%; /* 1/8th width on medium screens and up (half of 1/4) */
            }
        }
        /* Custom class for text container next to profile image */
        .profile-text-container {
             width: 100%; /* Full width on small screens (stacks) */
        }
         @media (min-width: 768px) { /* Corresponds to Tailwind's 'md' breakpoint */
            .profile-text-container {
                 width: 87.5%; /* 7/8th width on medium screens and up */
            }
        }
    </style>
     <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
</head>
<body class="bg-gray-100 text-gray-800">
    <nav class="navbar text-white p-4">
        <div class="container mx-auto flex justify-between items-center">
            <a class="text-xl font-bold" href="#">Jinyoon Kim's Webpage</a>
            <div>
                <ul class="flex space-x-4">
                    <li>
                        <a class="hover:text-gray-300" aria-current="page" href="#">Home</a>
                    </li>
                    </ul>
            </div>
        </div>
    </nav>

    <div class="container mx-auto p-6 mt-6 bg-white rounded-lg shadow-md">
        <div class="flex flex-col md:flex-row items-start">
            <div class="profile-img-container mb-4 md:mb-0 md:mr-6">
                <img src="assets/img/jinyoon-kim.png" class="w-full rounded-lg shadow" alt="Profile Image" onerror="this.onerror=null; this.src='https://placehold.co/300x300/e2e8f0/64748b?text=incoming';">
            </div>
            <div class="profile-text-container">
                <h4 class="text-2xl font-semibold mb-2">Jinyoon Kim</h4>
                <p class="mb-4 text-gray-700 leading-relaxed">
                    I am currently pursuing my Master's in Computer Science at the University of Virginia, advised by <strong>Professor Yen-Ling Kuo</strong>. I received my Bachelor's degree in Computer Science from Penn State University in 2024. 
                </p>
                <p class="mb-4 text-gray-700 leading-relaxed">
                    My research interests are at the intersection of <strong>Embodied AI, Computer Vision, and Language</strong>. I am particularly focused on developing intelligent robotic agents that can seamlessly interact with and assist humans in complex, photorealistic environments. By leveraging foundations in 3D vision, multimodal learning (LLMs/VLMs), and Reinforcement Learning, I aim to build robust decision-support systems for mobile manipulation and assistive robotics.
                </p>
                <p class="mb-4 text-gray-700 leading-relaxed">
                    Previously, I extensively worked on automated medical image analysis and self-supervised learning systems, publishing several papers on uncertainty-aware segmentation and spatially coordinated attention mechanisms. I am now translating these insights into the robotics domain to improve agent reliability and human-robot collaboration.
                </p>
                 <a href="assets/pdfs/CV_Jinyoon_Kim.pdf" target="_blank" class="link-button mr-4">CV</a>
                 <a href="https://github.com/jinyoonok2" class="link-button ml-4">GitHub</a> |
                 <a href="https://www.linkedin.com/in/jinyoon-kim-a7b3b5252/" class="link-button ml-4">LinkedIn</a>
            </div>
        </div>
        
        <div class="mt-8">
            <h4 class="text-xl font-semibold section-heading"><em>Publications</em></h4>
            <div class="space-y-4">
                 <ul class="list-disc list-inside space-y-4">
                    <li class="bg-blue-50 p-3 rounded-md border-l-4 border-blue-500">
                        <strong>Interactive Robotics Manipulation in Photorealistic Environments with Diffusion-Based Decision Support</strong>. On Progress<br>
                        <span class="text-sm text-gray-700 block mt-1">
                            Developing a robotics simulation framework for mobile manipulation in photorealistic environments, focusing on human-robot interaction and assistive robotics. The research utilizes diffusion-based policies and uncertainty-aware learning to enable more robust robot decision support for assisting humans in complex tasks.
                        </span>
                        <span class="text-sm text-gray-600">Jinyoon Kim. Master's Thesis, University of Virginia, 2026. Advised by Professor Yen-Ling Kuo.</span><br>
                        <span class="text-sm">[ <a href="https://github.com/jinyoonok2/InteractiveRobotics" class="link-button">code</a> ]</span>
                    </li>
                    <li>
                        <strong>YOLO-SCSA: Enhanced YOLOv8 with Spatially Coordinated Shuffling Attention Mechanisms for Skin Cancer Detection</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim, Tianjie Chen, Hien Nguyen, and Md Faisal Kabir. In Proceedings of <em>IEEE International Conference on Machine Learning and Applications</em> (ICMLA 2024), pp. 408-415, Dec 15, 2024.</span><br>
                        <span class="text-sm">[ <a href="assets/pdfs/ICMLA_2024_YOLO_SCSA.pdf" class="link-button">paper</a> | <a href="https://github.com/jinyoonok2/YOLO-SCSA" class="link-button">code</a> ]</span>
                    </li>
                    <li>
                        <strong>Automated Image Segmentation Using Self-Iterative Training and Self-Supervised Learning with Uncertainty Scores</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim, Tianjie Chen, and Md Faisal Kabir. In <i>Book of Recent Advances in Deep Learning Applications: New Techniques and Practical Examples</i>, Chapter 1, April 4, 2025.</span><br>
                        <span class="text-sm">[ <a href="assets/pdfs/YOLOv8_ADL_ICMLA_2023_Book_Chapter_Camera_Ready.pdf" class="link-button">paper</a> | <a href="https://github.com/jinyoonok2/YOLOv8-ADL_Renewed" class="link-button">code</a> | <a href="https://www.routledge.com/Recent-Advances-in-Deep-Learning-Applications-New-Techniques-and-Practical-Examples/Onyekpe-Palade-Wani/p/book/9781032944623" class="link-button">Book Weblink</a> ]</span>
                    </li>
                    <li>
                        <strong>Automated Data Labeling for Object Detection via Iterative Instance Segmentation</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim and Md Faisal Kabir. In Proceedings of <em>IEEE International Conference on Machine Learning and Applications</em> (ICMLA 2023), pp. 845-850, Dec 15, 2023.</span><br>
                        <span class="text-sm">[ <a href="assets/pdfs/YOLOv8_ADL_ICMLA_2023_Camera_Ready.pdf" class="link-button">paper</a> | <a href="assets/pdfs/YOLOv8_ADL_ICMLA_2023_poster.pdf" class="link-button">poster</a> | <a href="https://github.com/jinyoonok2/YOLOv8-ADL" class="link-button">code</a> ]</span>
                    </li>
                </ul>
            </div>
        </div>

        <div class="mt-8">
            <h4 class="text-xl font-semibold section-heading"><em>Projects</em></h4>
            <div class="space-y-6">
                <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                    <div class="w-full md:w-1/5 mb-2 md:mb-0">
                        <img src="assets/img/3d-gaussian-splatting-scene-edit.gif" class="project-img" alt="3D Gaussian Splatting Project" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                    </div>
                    <div class="w-full md:w-4/5">
                        <strong>Text-Guided 3D Scene Editing: Volumetric Removal & Generative Addition (2025)</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim, Sansshita Baskaran, Manvitha Sunireddy <span>[ <a href="assets/pdfs/3DCV_Final_Report.pdf" class="link-button">report</a> | <a href="assets/pdfs/3DCV_Final_Presentation.pdf" class="link-button">presentation</a> | <a href="https://github.com/jinyoonok2/3DCV-3D-Scene-Edit-with-3DGS" class="link-button">code</a> ]</span></span><br>
                        <ul class="list-disc list-inside text-sm text-gray-700 mt-1 space-y-1">
                            <li>Developed an end-to-end pipeline for editing unbounded Mip-NeRF 360 scenes using <strong>3D Gaussian Splatting</strong>.</li>
                            <li>Engineered a custom <strong>"Occlusion-Aware Lifting"</strong> algorithm to bridge 2D semantics (GroundingDINO + SAM 2) with 3D geometry, enabling precise zero-shot object selection.</li>
                            <li>Integrated <strong>LaMa</strong> for multi-view consistent inpainting (removal) and <strong>GaussianDreamer</strong> for inserting generative 3D assets (addition), solving the "Artichoke Problem" in volumetric editing.</li>
                        </ul>
                    </div>
                </div>

                 <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                    <div class="w-full md:w-1/5 mb-2 md:mb-0">
                        <img src="assets/img/rl-llm-project-image.png" class="project-img" alt="RL LLM Project" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                    </div>
                    <div class="w-full md:w-4/5">
                        <strong>A Reinforcement Learning Pipeline for Financial Reasoning (2025)</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim, Scarlett Yu, Donggen Li <span>[ <a href="assets/pdfs/RL_Final_Report.pdf" class="link-button">report</a> | <a href="assets/pdfs/RL_Final_Presentation.pdf" class="link-button">presentation</a> | <a href="https://github.com/jinyoonok2/RL-LLM-Reinforcement-Learning-Project" class="link-button">code</a> ]</span></span><br>
                        <ul class="list-disc list-inside text-sm text-gray-700 mt-1 space-y-1">
                            <li>Developed a comprehensive ablation study comparing <strong>Deep Contextual RL</strong> (PPO, GRPO, RLOO, DPO) against <strong>Heuristic Bandits</strong> for the FinQA task.</li>
                            <li>Engineered a <strong>"Discriminative Reranking"</strong> pipeline to bypass generation failures, successfully training Llama-3.2-3B and TinyLlama-1.1B models.</li>
                            <li>Demonstrated that RL provides massive gains for weak learners (<strong>+35% accuracy</strong> on 1B models) while establishing the "SFT Ceiling" effect on capable 3B models.</li>
                        </ul>
                    </div>
                </div>

                <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                    <div class="w-full md:w-1/5 mb-2 md:mb-0">
                          <img src="assets/img/skin-cancer-detection-interface1.png" class="project-img" alt="Skin Cancer Detection Project" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                    </div>
                    <div class="w-full md:w-4/5">
                        <strong>Capstone Project: Skin Cancer Detection Web Application (2024)</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim, Tianjie Chen, Hien Nguyen, and Md Faisal Kabir <span>[ <a href="assets/pdfs/Capstone-Final-Presentation.pdf" class="link-button">slides</a> | <a href="https://github.com/jinyoonok2/Skin-Cancer-Detection-Capstone" class="link-button">project code</a> | <a href="https://github.com/jinyoonok2/SkinWebApp" class="link-button">web app code</a> ]</span></span><br>
                        <ul class="list-disc list-inside text-sm text-gray-700 mt-1 space-y-1">
                            <li>Created an accessible and user-friendly web application for skin cancer detection using YOLOv8.</li>
                            <li>Utilized combined ISIC datasets, implemented confounding factors removal, and incorporated interpretability techniques.</li>
                            <li>Developed a PWA-based web application ensuring wide accessibility and transparent diagnostics through visual interpretability.</li>
                        </ul>
                    </div>
                </div>

                <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                     <div class="w-full md:w-1/5 mb-2 md:mb-0">
                          <img src="assets/img/face_recognition.png" class="project-img" alt="Face Recognition Project" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                     </div>
                     <div class="w-full md:w-4/5">
                         <strong>Machine Learning Project: Face Recognition Program (2023)</strong><br>
                         <span class="text-sm text-gray-600">Jinyoon Kim, Aditya Kendre, et al. <span>[ <a href="assets/pdfs/Face Classification ResNet.pdf" class="link-button">slides</a> | <a href="https://github.com/jinyoonok2/resNet-face-recognition" class="link-button">code</a> ]</span></span><br>
                         <ul class="list-disc list-inside text-sm text-gray-700 mt-1 space-y-1">
                             <li>Built a face recognition system focused on high accuracy and effective feature extraction.</li>
                             <li>Developed using a fine-tuned ResNet model and implemented a Top-k features algorithm.</li>
                             <li>Successfully created a model capable of accurately classifying team members via face recognition.</li>
                         </ul>
                     </div>
                 </div>

                <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                    <div class="w-full md:w-1/5 mb-2 md:mb-0">
                         <img src="assets/img/plantvildemo.png" class="project-img" alt="Plant Village Demo Project" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                    </div>
                    <div class="w-full md:w-4/5">
                        <strong>Plant Village Demo: ML Classification on Mobile Application (2023)</strong><br>
                        <span class="text-sm text-gray-600">Jinyoon Kim <span>[ <a href="https://github.com/jinyoonok2/PlantVilageDemo" class="link-button">code</a> | <a href="https://youtu.be/7hHgL_G_ou0" class="link-button">video</a> ]</span></span><br>
                        <ul class="list-disc list-inside text-sm text-gray-700 mt-1 space-y-1">
                            <li>Created a mobile application for detecting plant diseases using neural networks.</li>
                            <li>Developed and fine-tuned MobileNet specifically for plant disease detection on mobile devices.</li>
                            <li>The resulting application runs efficiently and accurately classifies plant disease images in a mobile environment.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <div class="mt-8">
            <h4 class="text-xl font-semibold section-heading"><em>Awards & Honors</em></h4>
            <div class="col-12">
                <ul class="list-disc list-inside space-y-2 text-gray-700">
                    <li>
                        National Ackroyd Healthier Days Scholarship, 2024. Awarded for a research project focused on improving health environments for patients through a skin cancer detection application. This project was conducted in collaboration with the Ackroyd Family Foundation and the Penn State community.
                    </li>
                    <li>
                        PennState Computer Science Department Undergraduate Student Award, 2023. Recognized by the Penn State Computer Science Department for my participation in the ICMLA 2023 conference, where I presented research on a self-supervised learning algorithm for an automated image segmentation system.
                    </li>
                </ul>
            </div>
        </div>

         <div class="mt-8">
            <h4 class="text-xl font-semibold section-heading"><em>News</em></h4>
             <div class="space-y-4">
                <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                    <div class="w-full md:w-1/5 mb-2 md:mb-0">
                         <img src="assets/img/psu_conference.png" class="project-img" alt="PSU Capstone Conference" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                    </div>
                    <div class="w-full md:w-4/5 text-sm text-gray-700">
                        Pennsylvania State University Capstone Project Conference 2024. May 1, 2024. I attended the Penn State Capstone Project Conference 2024 as a member of the Capstone Project Team. Thanks to Professor Nguyen, Professor Kabir, and my colleague Tianjie Chen.
                    </div>
                </div>
                <div class="flex flex-col md:flex-row items-start space-x-0 md:space-x-4">
                    <div class="w-full md:w-1/5 mb-2 md:mb-0">
                          <img src="assets/img/ICMLA2023-presentation2.png" class="project-img" alt="ICMLA Conference 2023" onerror="this.onerror=null; this.src='https://placehold.co/200x112/e2e8f0/64748b?text=incoming';">
                    </div>
                    <div class="w-full md:w-4/5 text-sm text-gray-700">
                         IEEE International Conference on Machine Learning and Applications (IEEE ICMLA 2023). December 15, 2023. I attended ICMLA 2023 with the poster of my paper for the presentation. Thanks for Dr. Kabir and everyone I met at the conference.
                        <span>[<a href="https://www.icmla-conference.org/icmla23/" class="link-button">ieee website</a>]</span>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <footer class="text-center text-gray-500 text-sm mt-8 pb-4">
        &copy; Jinyoon Kim
    </footer>

</body>
</html>